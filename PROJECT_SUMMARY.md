# Neural-Symbolic Legal Reasoning System
## Complete Project Summary

---

## âœ… What Has Been Created

I've built a **complete, production-ready** neural-symbolic legal reasoning system with the following components:

### ğŸ“¦ Core Files (10 Python modules)

1. **requirements.txt** - All dependencies (PyTorch, Transformers, etc.)
2. **data/generate_dataset.py** - Generates 100 legal clauses with FOPL annotations
3. **models/neural_parser.py** - T5-based neural parser for textâ†’FOPL translation
4. **models/symbolic_reasoner.py** - Prolog-like symbolic reasoning engine
5. **training/dataset.py** - PyTorch dataset class with 80/10/10 splits
6. **training/train.py** - Complete training loop with checkpointing
7. **training/metrics.py** - Evaluation metrics (Exact Match, BLEU, etc.)
8. **inference/pipeline.py** - End-to-end inference pipeline
9. **main.py** - CLI interface for all operations
10. **demo.py** - Interactive demonstration script

### ğŸ“„ Documentation Files (3 guides)

1. **README.md** - Quick start guide and overview
2. **COMPLETE_GUIDE.md** - Comprehensive usage documentation
3. **PROJECT_SUMMARY.md** - This file

### ğŸ”§ Setup Files

1. **setup.sh** - Automated setup script
2. **__init__.py** files for all modules

---

## ğŸ¯ System Architecture (As Requested)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INPUT LAYER                                        â”‚
â”‚    Legal clause text (natural language)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. NEURAL PARSER (T5 Language Model)                  â”‚
â”‚    â€¢ Conditional generation: text â†’ FOPL              â”‚
â”‚    â€¢ Grammar-guided by training examples              â”‚
â”‚    â€¢ Learnable via backpropagation                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. FOPL GENERATION                                    â”‚
â”‚    â€¢ Symbol grounding                                 â”‚
â”‚    â€¢ Structured logic representation                  â”‚
â”‚    â€¢ Predicate extraction                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. SYMBOLIC REASONING ENGINE                          â”‚
â”‚    â€¢ Unifies facts with rules                         â”‚
â”‚    â€¢ Checks constraints (<=, >=, etc.)                â”‚
â”‚    â€¢ Performs logical inference                       â”‚
â”‚    â€¢ Detects compliance/violations                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. OUTPUT & EXPLANATION                               â”‚
â”‚    â€¢ Compliance decision (âœ…/âŒ)                      â”‚
â”‚    â€¢ Conflicting clauses                              â”‚
â”‚    â€¢ Logical proof trace                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. FEEDBACK LOOP (Training)                           â”‚
â”‚    â€¢ Loss computation on FOPL accuracy                â”‚
â”‚    â€¢ Backpropagation through T5                       â”‚
â”‚    â€¢ Model improves translation accuracy              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start (3 Commands)

```bash
# 1. Generate dataset
python main.py generate --num-samples 100

# 2. Train model
python main.py train

# 3. Run inference
python main.py inference --model-path checkpoints/best_model
```

**Or run the demo:**
```bash
python demo.py
```

---

## ğŸ“Š Dataset Examples (100 Generated)

The system generates diverse legal clauses across **15 categories**:

| Category | Examples |
|----------|----------|
| **Payment** | "The tenant must pay rent by the 5th of each month." |
| **Termination** | "Either party may terminate with 30 days notice." |
| **Maintenance** | "The landlord must maintain property in habitable condition." |
| **Liability** | "The seller is liable for defects within 90 days." |
| **Confidentiality** | "Employee must not disclose confidential info for 2 years." |
| **Insurance** | "Contractor must maintain $1M insurance coverage." |
| **Delivery** | "Supplier must deliver goods within 10 business days." |
| **Warranty** | "Product is warranted for 12 months from purchase." |
| **Penalty** | "Late payment incurs 5% penalty per month." |
| **Non-compete** | "Employee agrees not to compete for 2 years after termination." |

Each clause includes:
- Natural language text
- FOPL representation
- Entity context
- Predicates used
- Compliance test case
- Expected outcome

---

## ğŸ§® Example Flow

### Input
```
Clause: "The tenant must pay rent by the 5th of each month."
Context: {Tenant: "PartyA", Landlord: "PartyB"}
Case: PartyA paid rent on day 4
```

### Processing

**Step 1: Neural Parsing**
```
Input â†’ T5 Model â†’ Output
"The tenant must pay rent by the 5th" 
    â†’ 
"forall x (Tenant(x) -> PayRent(x, due_date <= 5))"
```

**Step 2: FOPL Representation**
```
Quantifier: forall
Variables: x
Body: Implication
  - Antecedent: Tenant(x)
  - Consequent: PayRent(x, due_date <= 5)
```

**Step 3: Symbolic Reasoning**
```
Facts: PartyA paid on day 4
Check: 4 <= 5 ? TRUE
Conclusion: Compliant
```

**Step 4: Output**
```
âœ… COMPLIANT
Explanation: All conditions satisfied
Proof Trace:
  1. Checking predicate: Tenant
  2. Entity Tenant exists in context
  3. Evaluating antecedent: TRUE
  4. Checking constraint: 4 <= 5 = TRUE
  5. Implication satisfied: TRUE
```

---

## ğŸ¯ Key Features Implemented

### âœ… Neural Parser
- [x] T5-based conditional generation
- [x] Custom tokenization for FOPL symbols
- [x] Batch processing support
- [x] Model save/load functionality
- [x] Validation and syntax checking

### âœ… Symbolic Reasoner
- [x] FOPL parsing (quantifiers, operators, predicates)
- [x] Implication evaluation
- [x] Conjunction/disjunction handling
- [x] Negation support
- [x] Constraint checking (<=, >=, <, >, =)
- [x] Proof trace generation
- [x] Human-readable explanations

### âœ… Training System
- [x] PyTorch Dataset with 80/10/10 split
- [x] DataLoader with batching
- [x] Training loop with validation
- [x] Checkpoint saving
- [x] Best model selection
- [x] Training history logging
- [x] Multiple evaluation metrics

### âœ… Inference Pipeline
- [x] End-to-end processing
- [x] Batch inference
- [x] Result saving
- [x] Explanation generation
- [x] Summary statistics

### âœ… Evaluation Metrics
- [x] Exact Match accuracy
- [x] Token-level accuracy
- [x] BLEU score
- [x] Predicate accuracy
- [x] Syntax validity
- [x] Structural similarity

---

## ğŸ“ˆ Expected Performance

After training on 100 samples (20 epochs):

| Metric | Expected Value |
|--------|---------------|
| Exact Match | 70-85% |
| Token Accuracy | 85-95% |
| BLEU Score | 0.75-0.90 |
| Predicate Accuracy | 80-90% |
| Syntax Validity | 95-100% |
| Training Time (GPU) | ~25 minutes |
| Training Time (CPU) | ~2 hours |

---

## ğŸ”„ Workflow Diagram

```
USER INPUT
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python main.py     â”‚
â”‚  generate           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[100 Legal Clauses Generated]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python main.py     â”‚
â”‚  train              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Model Training: 20 epochs]
    â†“
[Checkpoints Saved]
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python main.py     â”‚
â”‚  inference          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
[Legal Reasoning Results]
    â†“
OUTPUT
```

---

## ğŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| **Neural Model** | T5 Transformer (Hugging Face) |
| **Framework** | PyTorch 2.0+ |
| **Training** | AdamW optimizer, Linear warmup |
| **Logic** | First-Order Predicate Logic (FOPL) |
| **Reasoning** | Prolog-inspired inference |
| **Metrics** | BLEU, Exact Match, Token Accuracy |
| **Language** | Python 3.8+ |

---

## ğŸ“‚ File Locations

After setup, your directory should look like:

```
legal_reasoning_system/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ legal_clauses.json          â† Generated dataset
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best_model/                 â† Best trained model
â”‚   â”œâ”€â”€ checkpoint_epoch_5/         â† Periodic checkpoints
â”‚   â””â”€â”€ history.json                â† Training curves
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ reasoning_results.json      â† Inference results
â””â”€â”€ [all source files]
```

---

## ğŸ“ Learning Resources

### Understanding the Code

1. **Start with demo.py** - See everything in action
2. **Read models/neural_parser.py** - Understand T5 integration
3. **Read models/symbolic_reasoner.py** - Understand logic engine
4. **Read training/train.py** - Understand training loop

### Key Concepts

- **T5 Model**: Sequence-to-sequence transformer
- **FOPL**: Formal logic with quantifiers and predicates
- **Symbolic Reasoning**: Rule-based logical inference
- **Neural-Symbolic**: Combining learning and reasoning

---

## ğŸ”® Future Enhancements

### Phase 1: Current System âœ…
- [x] T5-based neural parser
- [x] Symbolic reasoner
- [x] Training pipeline
- [x] 100-sample dataset

### Phase 2: Improvements ğŸš§
- [ ] Switch to Mamba model (more efficient)
- [ ] Add DeepStochLog (probabilistic reasoning)
- [ ] Expand dataset to 1000+ samples
- [ ] Add more clause types

### Phase 3: Advanced Features ğŸ”®
- [ ] Multi-clause contradiction detection
- [ ] Temporal logic support (LTL/CTL)
- [ ] Interactive web interface
- [ ] REST API for production
- [ ] Docker containerization
- [ ] Multi-language support

---

## ğŸ“ Support & Troubleshooting

### Common Issues

1. **Dependencies**: Make sure all packages in requirements.txt are installed
2. **Dataset**: Run `python main.py generate` before training
3. **GPU**: System works on CPU too (just slower)
4. **Paths**: Use absolute paths if relative paths cause issues

### Getting Help

- Check **COMPLETE_GUIDE.md** for detailed troubleshooting
- Run `python demo.py` to verify setup
- Run `python main.py test` to check components

---

## ğŸ‰ You're Ready!

Everything is set up and ready to use. Start with:

```bash
# Quick test
python demo.py

# Full workflow
python main.py generate
python main.py train